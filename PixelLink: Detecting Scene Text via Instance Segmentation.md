## 摘要

目前的大多数sota文本检测模型都是基于文本/非文本分类+边框回归的方法。另一方面文本检测可以看成是一个图像分割问题，但是因为现实问题中的文本通常距离非常近，直接做图像分割非常容易产生粘连。Pixel通过预测像素之间的连接关系来完成实例分割，能够在更少的数据和更少的迭代次数下获得较好的效果。

## 1. 简介

目前较好的文本检测算法有：CTPN、TextBoxes、Seglink、EAST。这些算法通常有两个预测任务：

（1） 文本非文本分类
（2）边框回归，EAST预测绝对位置，其他预测与Anchor box的偏移。
预测完成之后再通过后期处理方法（NMS或者SegLink中的预测链接）得到最终检测框。

这些方法里面都用到了边框回归，但是作者认为边框回归并不是必不可少的内容，因为从图像分割的结果中可以直接得到检测框。

为了解决前面提到的文本实例相距太近的问题，在PixelLink算法中，神经网络有两个预测任务：文本/非文本预测+连接预测

文本预测：实例内的像素为Positive，实例外的为Negative

连接（Link）的概念来自于SegLink，但是有明显区别。每个像素有八个邻域（Neighbors），对于一个像素和它的相邻像素，如果它们都在一个实例中，则它们之间的Link被标记为Positive，否则Negative。

通过文本预测结果+连接预测结果可以将像素连接成连通域，然后通过Opencv 中的minAreaRect找到包含连通域的最小矩形，就可以达到边框预测的结果。

## 2. 实现

### 2.1 网络结构

特征提取使用了VGG16，与SSD/SegLink等网络相同。其中的全连接层fc6/fc7呗换成了卷积层。

网络有两个预测Header，都使用了softmax，文本预测结果有1*2=2个网络，连接预测结果有8*2=16个结果。

实验设计了两种特征融合方式，分别融合{conv2 2, conv3 3, conv4 3, conv5 3, fc 7}和{conv3 3, conv4 3, conv5 3, fc 7}，名为Pixellink + vgg16 2s 和 Pixellink + vgg16 4s，其中2s结果是是原图的一般，4s是四分之一。

### 2.2 连接像素

得到像素之后，通过Positive像素进行连通域连接，两个像素之间的连接通过二者共同判断，只要有一个为正，就可以视为有效连接。

连接方式采用并查集的方式。

### 2.3 提取边界框

边界框直接用opencv中的minarearect进行提取。

### 2.4 分割前的预筛选

通过几何学特征去除噪点，这个需要根据数据集而定。

## 3. 一些细节


### 3.1 标记


标记像素时，如果两个框有重叠，则只标记没有重叠的部分，重叠部分标记为Negative

注意输出图和输入图的尺寸不一样

### 3.2 损失函数

L = \lambda L_pixel + L_link

\lambda 取2.0

计算像素loss的时候，文本的大小会对loss造成很大影响。为了解决这个问题，引入了Instance-Balanced Crossed-Entropy Loss。这种方法会根据实例的大小给予不同的权重。

计算loss还使用了OHEM方法来选择negative 像素，只选择r*s个loss最大的negative像素，像素权重设为1。

计算link loss，通过像素的权重计算link权重。

数据增强采用了类似SSD的方式。

5. 训练细节

数据：ICDAR 2013+ ICDAR2015+MSRA-TD500

优化器：SGD/momentum=0.9/weight_decay=5e-4

学习率：(1e-3,100),(1e-2,rest)

框架：Tensorflow 1.1

batch_size:24

使用3块TitanX训练7-8小时。

